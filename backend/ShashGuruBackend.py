#This file is part of ShashGuru, a chess analyzer that takes a FEN, asks a UCI chess engine to analyse it and then outputs a natural language analysis made by an LLM.
#Copyright (C) 2025  Alessandro Libralesso
#
#This program is free software: you can redistribute it and/or modify
#it under the terms of the GNU General Public License as published by
#the Free Software Foundation, either version 3 of the License, or
#(at your option) any later version.
#
#This program is distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#GNU General Public License for more details.
#
#You should have received a copy of the GNU General Public License
#along with this program.  If not, see <https://www.gnu.org/licenses/>.

from flask import Flask, request, Response, stream_with_context, jsonify, json
from flask_cors import CORS
import argparse
import logging 


import LLMHandler
import engineCommunication

app = Flask(__name__)
CORS(app)


@app.route("/analysis", methods=['GET', 'POST'])
def analysis():
    fen = request.json.get('fen')  
    print("Received analysis request for:", fen)
    depth = 15
    
    bestmoves, ponder = engineCommunication.call_engine(fen, depth, lines=3)
    prompt = LLMHandler.create_prompt_single_engine(fen, bestmoves, ponder)

    ############################
    #  Questo Ã¨ per due engine #
    ############################
    #engine_analysis = engineCommunication.engines(fen, depth)
    #prompt = LLMHandler.create_prompt_double_engine(fen, engine_analysis)

    def generate():
        yield json.dumps({"prompt": prompt}) + "\n[PROMPT_END]\n" # sends single chunk with prompt, to save it as context for responses
        yield "[START_STREAM]\n"  # optional: delimiter for stream start
        for token in LLMHandler.stream_LLM(prompt, model):  # <-- stream here
            yield token
        yield "\n[END_STREAM]"  # optional: delimiter for stream end 
    
    return Response(stream_with_context(generate()), mimetype='text/plain')


@app.route("/response", methods=['GET','POST'])
def response():
    chat_history = request.get_json()
    if chat_history is None:
        chat_history = [] 
    new_question = chat_history[-1].get("content")
    print('Received question:' , new_question)

    def generate():
        yield "[START_STREAM]\n"
        for token in LLMHandler.stream_LLM(new_question, model, chat_history=chat_history[:-1]):
            yield token
        yield "\n[END_STREAM]"

    return Response(stream_with_context(generate()), mimetype='text/plain')


@app.route("/evaluation", methods=['GET', 'POST'])
def evaluation():
    """
    Returns engine evaluation for a given FEN position.
    
    Expected JSON payload:
    {
        "fen": "string",
        "depth": int (optional, default: 15),
        "lines": int (optional, default: 3)
    }
    
    Returns JSON with evaluation data including score, best moves, etc.
    """
    try:
        data = request.get_json()
        fen = data.get('fen')
        depth = data.get('depth', 15)
        lines = data.get('lines', 3)
        
        if not fen:
            return jsonify({"error": "FEN is required"}), 400
            
        print(f"Received evaluation request for FEN: {fen}, depth: {depth}, lines: {lines}")
        
        # Get engine analysis
        bestmoves, ponder = engineCommunication.call_engine(fen, depth, lines=lines)
        
        # Format the response to match frontend expectations
        evaluation_data = {
            "evaluation": {
                "move": bestmoves[0]['move'] if bestmoves else None,
                "score": bestmoves[0]['score'] if bestmoves else None,
                "mate": bestmoves[0]['mate'] if bestmoves else None,
                "winprob": bestmoves[0]['winprob'] if bestmoves else None,
                "lines": [
                    {
                        "moves": ' '.join(move_data['pv_moves']) if 'pv_moves' in move_data and move_data['pv_moves'] else move_data['move'],
                        "evaluation": move_data['score'] if move_data['score'] is not None else (1000 + move_data['mate'] if move_data['mate'] is not None else 0)
                    }
                    for move_data in bestmoves[:lines]
                ]
            }
        }
        
        return jsonify(evaluation_data)
        
    except Exception as e:
        print(f"Error in evaluation endpoint: {e}")
        return jsonify({"error": str(e)}), 500



    





if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Run ShashGuru Backend with optional flags.")
    parser.add_argument("--M", action="store_true", help="Use MATE model")
    parser.add_argument("--S", action="store_true", help="Use Llama3.2-1B model")
    parser.add_argument("--L", action="store_true", help="Use Llama3.1-8B model")
    args = parser.parse_args()

    modelNumber = 1
    if args.L:
        modelNumber = 1
    elif args.S:
        modelNumber = 2
    elif args.M:
        modelNumber = 3
    else: 
        model_number = 1

    tokenizer, model = LLMHandler.load_LLM_model(modelNumber)
    logging.info("Loaded model.")
    #THIS IS NECESSARY, DO NOT REMOVE
    app.run(host="0.0.0.0", port=5000, debug=True)
    
