version: '3.9'

services:
  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    ports:
      - "80:80"
      - "6666:6666"
    restart: unless-stopped
    volumes:
      - ./config/nginx_vllm.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - model_server_0
      - model_server_1
      - model_server_2
      - model_server_3
      - model_server_4
      - model_server_5
      - backend

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    ports:
      - "5000:5000"
    restart: unless-stopped
    volumes:
      - ./backend:/app
    environment:
      - FLASK_ENV=development
      - REDIS_HOST=redis_cache
      - REDIS_PORT=6379
    depends_on:
      - redis

  model_server_0:
    image: shahguru_model_server:latest
    container_name: model_server_0
    expose:
      - 8000
    cpuset: "0-42,256-298"
    restart: unless-stopped
    mem_limit: 64g
    shm_size: 16g
    volumes:
      - ./models:/llm/models
    environment:
      - PORT=8000
      - QUANTIZATION=sym_int4
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
    command: ./start_server.sh

  model_server_1:
    image: shahguru_model_server:latest
    container_name: model_server_1
    expose:
      - 8000
    cpuset: "43-84,299-340"
    restart: unless-stopped
    mem_limit: 64g
    shm_size: 16g
    volumes:
      - ./models:/llm/models
    environment:
      - PORT=8000
      - QUANTIZATION=sym_int4
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
    command: ./start_server.sh

  model_server_2:
    image: shahguru_model_server:latest
    container_name: model_server_2
    expose:
      - 8000
    cpuset: "85-127,341-383"
    restart: unless-stopped
    mem_limit: 64g
    shm_size: 16g
    volumes:
      - ./models:/llm/models
    environment:
      - PORT=8000
      - QUANTIZATION=sym_int4
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
    command: ./start_server.sh

  model_server_3:
    image: shahguru_model_server:latest
    container_name: model_server_3
    expose:
      - 8000
    cpuset: "128-170,384-426"
    restart: unless-stopped
    mem_limit: 64g
    shm_size: 16g
    volumes:
      - ./models:/llm/models
    environment:
      - PORT=8000
      - QUANTIZATION=sym_int4
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
    command: ./start_server.sh

  model_server_4:
    image: shahguru_model_server:latest
    container_name: model_server_4
    expose:
      - 8000
    cpuset: "171-212,427-468"
    restart: unless-stopped
    mem_limit: 64g
    shm_size: 16g
    volumes:
      - ./models:/llm/models
    environment:
      - PORT=8000
      - QUANTIZATION=sym_int4
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
    command: ./start_server.sh

  model_server_5:
    image: shahguru_model_server:latest
    container_name: model_server_5
    expose:
      - 8000
    cpuset: "213-255,469-511"
    restart: unless-stopped
    mem_limit: 64g
    shm_size: 16g
    volumes:
      - ./models:/llm/models
    environment:
      - PORT=8000
      - QUANTIZATION=sym_int4
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
    command: ./start_server.sh

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: redis_cache
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --appendfsync everysec
    restart: unless-stopped


volumes:
  grafana-storage:
  redis-data:
